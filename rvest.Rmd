# rvestでスクレイピング {#rvest}

## スクレイピング

ここでのスクレイピングとは，ウェブスクレイピングの省略のことで，ウェブサイトにある情報を収集することである．
ウェブサイトから植生調査データを収集することはほとんどないものの，関連データの収集は可能である．
例えば，気象庁のページから気象データが収集可能である．
もちろん，気象データは手動でも収集可能ではあるが，多大な手間と長い時間が必要である．
研究に必要なデータを自動で取得できれば，手間と時間の節約が可能である．

そこで，本稿ではウェブでの情報収集の方法を紹介することを目的とする．
世界の各地点の気象データをプロット
情報収集にはRのパッケージであるrvestを用いる．
rvestを用いて気象庁のページから世界の気象データを入手して，気候ダイアグラムを描画する．


Rのパッケージ作成では，rvestを用いて作成した関数と収集したデータをまとめたパッケージの作成方法を紹介する．
著者自身，他人のためにパッケージをつくることは考えておらず，基本的には自分の研究や作業のための関数をまとめることを目的としてパッケージをいくつか作成した．
作成したら，ついでに他人にも使ってもらえれば嬉しいという程度である．

過去に作成した関数は，しばらくすると関数の引数や返り値がどのようなものであったのか忘れてしまいがちである．
パッケージをつくる(特にCRANに登録する)には，引数，返り値，使用例などをまとめる必要がある．
きっちりまとめなくても良いのではあるが，決まった形式の方がむしろまとめやすい．
また，RStudioとusethis, testthat, devtoolsなどのパッケージを使ってパッケージ開発すると，各種チェックやテストが可能である．
各種チェックやテストでたくさんのエラーを見ると，チェックやテストは正直なところ煩わしいと感じる．
特に，パケージ開発に慣れていないと特にそうである．
しかし，チェックやテストをすることで，関数の完成度を確実に高めることができるため，パッケージとしてまとめる利点である．


## rvest と RSelenium

スクレイピングをするために使われる主なRのパッケージとしては，rvestとRSeleniumがある．
rvestは，静的なサイトを対象とするときに役立つ．
つまり，URLを指定すれば対象のサイトのページが決まるときである．
気象庁での気象データを提供しているページがこれに当たる．
一方，RSeleniumは動的なサイトを対象とするときに役立つ．
例えば，テキストボックスへのデータ入力やプルダウンメニューの選択あるいはその後のマウス操作でページが遷移する場合である．
このような動的なサイトでは，Seleniumだけでなく，Javascriptを部分的に用いるのも効果的である．
なお，rvestでもユーザ名とパスワードを用いた一般的なログインは可能である．
また，politeパッケージと組み合わせることである程度の動的なサイトのスクレイピングは可能である．

<!--
CRANには，植生データの取得のためのパッケージがある．
  # 本当?
  # 海外のもの
しかし，自分のもとめるデータがあるとは限らない．
特に日本のデータの取得は少ない．

--> 

## rvestのできること

- HTMLの取得    
- DOMの取得: id, class, tagNameなどを用いる   
- tableの取得    
  - HTML内の取得したいデータはtableにあることが多いため，非常に便利
  そもそも，tableでないデータを取得するのは非常に不便
- リンクの取得  
  ページ遷移に使用する   
  - stringrと組み合わせて使うと良い   
  - 文字コードの変換にはstringiを用いる   
  - tidyverseやmagrittrとの合せ技が便利    
- Formの入力・選択
  - radioボタンはちょっと工夫が必要   
  - moranajp::html_radio_set()   
    無理やりな感じではあるが，同一名称のradioボタンを全て同じ値に変更する   
    本来なら，不要なradioボタンのフォームを削除   
      可能だが，インデクスがずれるので結構厄介  
- politeパッケージとの連携   
  使えば便利だが，ここでは説明せず

## 準備

例によってrvestをインストールする．
curlとpoliteパッケージは少しだけ使うので予めインストールしておく．
tidyverseは既にインストールしているはずだが，まだの場合はインストールする．

```{r eval = FALSE}
install.packages("rvest")
install.packages("curl")
install.packages("polite")
  # install.packages("moranajp")
  # install.packages("tidyverse") # 未インストールの場合
```

```{r message = FALSE}
library(rvest)
library(tidyverse)
```

## HTMLの取得

スクレイピングによってデータを取得するには，取得したいページのURLを特定しなければならない．
静的なページあるいは固定されたURLであれば，ブラウザのアドレスバーにあるURLをそのまま使えば良い．
動的あるいは特定の規則に従ったURLであれば，取得したいページのURLの規則性を知らなければならない．

ここでは，「日本のレッドデータ検索システム」から都道府県のRDB指定状況とその地図情報の画像を入手することを考える．

http://jpnrdb.com/search.php?mode=spec   

まずはブラウザでページにアクセス，手作業で検索，指定状況とその地図情報の画像を入手してみる．
上記URLで例として示されているニッコウキスゲをキーワード(種名)として入力すると，ページ遷移する．
アドレスバーにはカタカナがそのまま表示されている．
しかし，アドレスをコピーしてテキストエディタに貼り付けると文字化けしたようになる．

http://jpnrdb.com/search.php?mode=key&q=ニッコウキスゲ   
http://jpnrdb.com/search.php?mode=key&q=%E3%83%8B%E3%83%83%E3%82%B3%E3%82%A6%E3%82%AD%E3%82%B9%E3%82%B2

これはURLエンコードによってコード変換された結果であるが安心して欲しい．
rvestを使ってHTMLを取得するときには，日本語をそのまま使用することができる．
上記のURLのうち「http://jpnrdb.com/search.php?mode=」まではここで使用するページに共通する部分であるため，mainとしておく．
検索したい種名は変更する部分で，とりあえずspに入れておく．
キーワード検索の命令(phpによるクエリ)と種名の文字列を結合し，さらにmainと結合する．
これで得たURLをread_html()に与えると，ページのHTMLを得ることができる．

```{r}
main <- "http://jpnrdb.com/search.php?mode="
sp <- "ニッコウキスゲ"
find_sp <- paste0("key&q=", sp)
html <- 
  paste0(main, find_sp) %>%
  rvest::read_html()
html
```

## 必要な情報の取得

取得したHTMLには必要な情報が含まれているが，そのままの状態では使い物にならない．
また，文字列に変換してstringrを駆使すれば，情報を得ることはできるだろうが，多大な苦労が待っている．

```{r}
as.character(html)
```

幸いにしてrvestにはhtmlから要素を取得するための便利な関数が用意されている．
ニッコウキスゲを検索した結果のページとその後のページの内容とURLを見ると次のような規則性があることに気づく．
検索結果のページには表(table)としてデータが含まれており，その表の中の目録Noである「5259」が指定状況や地図のページのURLに含まれている．
つまり，目録Noを入手すれば指定状況や地図ページのURLを生成できる．

HTMLのtableを取得するには，html_table()を使う．
ここでは，6個目のtableが目録Noを含んでいる．

```{r}
rvest::html_table(html)
```

```{r}
rvest::html_table(html) %>%
  `[[`(6)
```
和名としてゼンテイカとニッコウキスゲの2つが示されている．
ゼンテイカはニッコウキスゲの別名である．
生物学的には同じものなので，本来は両方の情報を合わせる必要がある．
別名かどうか判定するには生物の種について考える必要があり，この問題はかなり根深くてややこしいため，ここではあえて立ち入らない．
単純に検索したものと同じ文字列の和名の目録Noを得ることを考える．

tableの列名とその内容をもとにして目録Noを取得するには，dplyrのselectとfilterが便利だ．
selectは列名を指定する以外に列番号を指定できるので，それを使う．
さらに，filter，stringr，stringiの関数の合せ技でspと同じ文字列のnoを取り出す．

途中でちょっと面倒な点があるので補足する．
separateのsep(区切り文字)としてstringi::stri_unescape_unicode("\\u00a0")を指定している．
これは，普通の半角スペースに見えるが，No-Break Spaceと言われる改行を防ぐ特殊なスペースである．
これをそのままコードに入力しても良いが，どう見ても普通のスペースと見分けがつかない．
後からコードを書く時に普通のスペースを使ってしまうと，区分しようとしてもうまくいかない．
そこで，これは普通のスペースではないことを明示的に示した．
また，str_detectの引数で，paste0("^", sp, "$")としたのは，「ニッコウキスゲ」以外にマッチさせないためである．
例えば，「ギンラン」を検索すると，「ギンラン」以外にも「エゾギンラン」と「ササバギンラン」も出てくる．
この場合に正規表現の「^」(行頭の意味)「$」(行末の意味)を使うことで，「ギンラン」にしかマッチさせない．

```{r}
no <- 
  rvest::html_table(html) %>%
  `[[`(6) %>%
  dplyr::select(no = 1, wamei = 4) %>%
  tidyr::separate(wamei, into = "wamei", sep = stringi::stri_unescape_unicode("\\u00a0"), extra = "drop") %>%
  dplyr::filter(stringr::str_detect(wamei, paste0("^", sp, "$"))) %>%
  `[[`("no")
```

## URLの生成・データの取得

目録Noが取得できれば，完成したようなものである．
ブラウザで表示した地図や指定状況のURLを生成する．
"map&q=0605009"の詳細な意味はよくわからないが，"0605009"あたりは分類群を指定しているのだと考えられる．
これに維管束植物(コケなどを除くシダ植物と花の咲く植物)の中での目録Noを結合して，さらにmainを結合するとURLの出来上がりだ．

```{r}
show_sp <- paste0("map&q=0605009", no)
paste0(main, show_sp)
html <- 
  paste0(main, show_sp) %>%
  rvest::read_html()
```


生成したURLをブラウザで表示させると地図ページが表示される．
一覧表の表示にしてもURLは変更されないため，内部的に表示を変更させている可能性が高い．
そこで，とりあえずHTMLからtableデータを取得してみる．

```{r}
html %>%
  rvest::html_table()
```

果たして，tableの4番目に欲しいデータがあった．
あとは，filterを使って指定されていない都道府県データを除去する．
さらに必要に応じて，入手したデータを整形・変換・保存して欲しい．

```{r}
html %>%
  rvest::html_table() %>%
  `[[`(4) %>%
  dplyr::filter(和名 != "-")
```

## 地図画像の取得

指定状況の地図画像を取得するには，まずブラウザで画像のURLを得る必要がある．
GoogleChromeで画像を右クリックして，「画像アドレスをコピー」を選択する．
ニッコウキスゲの場合は，以下のURLを得ることができる．

http://jpnrdb.com/png/06/06050095259.png

指定状況の一覧表データのHTMLにも(ほぼ)同じものが含まれているはずである．
rvestで目的とするファイルのURLを得るコードは以下のとおりである．

```{r}
html %>%
  rvest::html_elements("img") %>%
  rvest::html_attr("src") %>%
  `[`(., stringr::str_detect(. , as.character(no)))
```

上のコード使用したように，rvestで便利な関数としてhtml_elements()とhtml_attr()がある．
それぞれ次のようにid，class，tag，属性によってHTMLからDOMを取得可能である．

- html_elements()   
  - html_elements("#id")   
  - html_elements(".class")   
  - html_elements("tag")    
- html_attr("attribute")   

DOMとはドキュメントオブジェクトモデルのことで，HTMLの各要素をオブジェクトとするモデルのことである．
id，class，tag，属性を指定することで，効率的にオブジェクトを取り出すことができて，便利である．

id，class，tag，属性についての詳細は，HTMLの解説などを別途参照していただきたい．
簡単に説明をすると，idはHTML内で一意に決定できるもので，日本のレッドデータ検索システムでは<id = "header">などが使われている．
classは，HTML内で複数出てくることがあり，<class = "kind_list">のように指定される．
tagは，上記の<id>や<class>を含めたすべてのタグのことで，他にも<p>，<div>，<table>など多くの物がある．
html_table()はhtml_elements("table")と同等であるが，tableタグは入手したいデータを含むことが多いため個別の関数が作成されたのだろう．
属性はtagの，「href = "index.html"」の部分で，html_attr("href")とすると，"index.html"を取り出すことができる．
hrefが複数ある場合は，すべてを含むベクトルが返り値になる．


ただし実際には，上のようにブラウザでの右クリックか，以下の手順で実行するのが手っ取り早い．
- ブラウザで地図ページを表示させる   
- F12を押して開発者ツールを開く   
- 左上の□と↖の結合したアイコンをクリック後に画像をクリック   
- Elementsのところに出てきたURLが求めるURL   
- タグを右クリックして[Copy] - [Copy element] や [Copy outerHTML] で内容をコピーできる  

画像のURLがわかれば，ファイルをダウンロードして保存するだけだ．
これは，curlパッケージのcurl_download()で簡単にできる．
引数としてurlにはURLを，destfileにはダウンロード後のファイル名を指定する．
ファイル名自体は指定が必要である．
パスを指定しないと作業ディレクトリ(getwd()で取得可能)に保存されるが，作業ディレクトリ以外に保存したい場合は，相対パスや絶対パスを指定する．

```{r}
  # wd <- "set_your_directory"
  # setwd(wd)
url_img <- "http://jpnrdb.com/png/06/06050095259.png"
curl::curl_download(url = url_img, destfile = paste0(sp, ".png"))
```

このようにしてスクレイピングが可能ではあるが，URLの生成規則は，変更されることがある．
read_html()でHTMLが取得できない場合は，URLが正しいか確認する必要がある．
また，動的なサイトでは，idが固定ではない可能性がある．
サイトの仕様変更によって，タグ，クラス，その他の構造が変更されることがある点も注意しなければならない．

綺麗な構造のサイトであっても，手作業が混入していることはある．
例えば，括弧が正しく対応しているはずだと思っていても，開く側が"『"で閉じる側が"」"になっていることがあった．
その場合に正規表現"『.+』"ではうまく鉤括弧内の文字列を取得できないことになる．

## 複数種への対応

前節のようにすれば，レッドデータへの指定状況とその地図データを得ることができる．
1種だけのデータ・画像の入手方法を紹介したが，複数種についてもこれを応用すれば可能である．
その際には，forループか，purrr::mapを使うと良いだろう．

複数ページのデータを取得する場合は一般的には5秒程度の間隔を置く必要がある．
ただし，サイトによってはそれ以上の間隔を求めているときがある．
その内容はドメインのトップに置かれた「robots.txt」で確認できる．
「http://jpnrdb.com/」には「robots.txt」が置かれていないが，politeパッケージの関数bow()でスクレイピングについて調べてみる．

```{r}
polite::bow("http://jpnrdb.com/")
```

「Crawl delay: 5 sec」とあるため，5秒間隔を求めていることが分かる．
これ未満の間隔でデータを頻繁に求めると，「攻撃」と見なされて接続できな状態になる可能性がある．
さらに，悪質なときには法的手段を取られることもありえるので，注意が必要である．


## Amazon Primeの新着情報の取得例



```{r}
Sys.setlocale("LC_TIME", "en_US.UTF-8") # アメリカ英語に設定
  # date <- lubridate::today()
date <- lubridate::ymd("2022-5-1")
ym <- paste0("^", year(date), "年", month(date) , "月")

main <- "https://www.aboutamazon.jp/news/entertainment/amazon-prime-video-new-content-"
url_amz <- 
  month(date, label = TRUE, abbr = FALSE) %>%
  stringr::str_to_lower() %>%
  paste0(main, ., "-", year(date))
html <- rvest::read_html(url_amz)
```


```{r}
polite::bow("https://www.aboutamazon.jp/")
```

```{r}
contents <- 
  html %>%
  rvest::html_elements("body") %>%
  rvest::html_elements("div.RichTextArticleBody-body li,p,h3.cms-headings-h3") %>%
  rvest::html_text() %>%
  tibble::as_tibble() %>%
  dplyr::filter(value != "")

contents %>%
  dplyr::mutate(
    div = dplyr::case_when(
      stringr::str_detect(value, "^洋画|邦画|アニメ|海外|国内|韓国") & stringr::str_length(value) < 20 ~ value,
      TRUE ~ NA    )) %>%
  dplyr::mutate(
    date = dplyr::case_when(
      stringr::str_detect(value, ym) ~ value,
      TRUE ~ NA    )) %>%
  tidyr::fill(all_of(c("div", "date")), .direction = "down") %>%
  dplyr::mutate(value = stringr::str_replace(value, "^Amazon Original", "")) %>%
  dplyr::mutate(value = stringr::str_replace_all(value, " ", "")) %>%
  dplyr::mutate(value = stringr::str_replace(value, "※.+", "")) %>%
  dplyr::filter(stringr::str_detect(value, "^『")) %>%
  print(n=100)


  #   html_elements("h3.cms-headings-h3") %>%

  # id         rvest::html_elements("#content") %>%
  # class      rvest::html_elements(".next") %>%
  # tag        rvest::html_elements("a") %>%
  # 属性       rvest::html_attr("href")
```
