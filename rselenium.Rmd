# Rseleniumでスクレイピング {#rselenium}

Seleniumは，ブラウザを使って動的に巡回しつつ，スクレイピングをするのに適している．

JavascriptやPHPなどを使って，動的に作成されるサイトでは，URLだけではページを特定することはできない．
そのため，rvestだけではデータを取得するのが困難である．

## 準備


- RSelenium: CRANからインストール   
- Selenium: 本家サイトからインストール    
  - 注意: ver3.xxx をインストールする    
  ver4.0 以上はRSeleniumが対応していない(Pythonなら可)    
  ver3の最終版は以下からダウンロード可能   
  https://github.com/SeleniumHQ/selenium/releases/tag/selenium-3.150.0   
  https://github.com/SeleniumHQ/selenium/releases/download/selenium-3.150.0/IEDriverServer_x64_3.150.2.zip   
- ブラウザのDriver   
  GoogleChromeの場合   
  https://chromedriver.chromium.org/downloads
  - 注意: 自身の利用しているブラウザのドライバが必要(バージョンも合致させる)   
  ブラウザが自動updateしていることがあるので，バージョンは要確認   
  - Seleniumと同じフォルダに保存する   

```{r eval = FALSE}
install.packages("RSelenium")
```

```{r, message = FALSE}
library(tidyverse)
library(RSelenium)
```



## ブラウザの自動化



## 

### 使い方


```{r}
```

- Rからシェルのコマンドを使う    
  - Seleniumuの起動・終了    


### 注意点

## 要素の取得

idがわかるとき
document.getElementByID()

xpath
document.selectQueryAll()[]
動的にサイトが作られているときには，変化する可能性があるので注意


使用されているJavaScriptの関数がわかる
script <- ""
rem$excute(script)

例
- BiSSの文字サイズの変更   
- 種名リストの列数の変更   

スクレイピングの実行時には，適切な間隔を空ける．

- 通常は5秒以上を求めていることが多い   
- 最低でも1秒は空ける   

スクレイピング時に適切な間隔を空けるのは，サーバ負荷の軽減だけでなく，実務的な意味合いもある．
ページ遷移の命令を送信後，十分な間隔がないとHTMLの要素を取得しきれていないことがある．
極端な場合，サーバーからの情報がほとんど何も送られていない，つまりページの内容がほとんど何もないことにある．
この状況は，通常のマウス操作では何も表示されていないところをクリックするのと同じ状態である．
サーバからの情報を待つ意味でも適度な間隔を空けるのが望ましい．


動的なサイトの場合は，HTMLの構成中の可能性もある．
ログイン等のページでも，遷移途中のことがある．


# Rseleniumでスクレイピング {#rselenium}

```{r, echo = FALSE, eval = FALSE}
  # 
  # RSeleniumあ必要だと思っていたけど，
  #   rvest::read_html_live()で色々できるかもしれない
  #   ただし，2024-04-01時点では，エラーが出てくる．
  # hadleyによると，それほどむつかしい問題ではなさそうとのこと
  #   https://github.com/tidyverse/rvest/issues/405
  # なので，しばらく待っておいて，エラーが解消されるなら，rvestでいく
  # 2024-05になっても，だめなら，RSeleniumで書くことを考える
  # 
  # 
  # 読者視点でのリード文を書く
  #   日常業務での面倒なことなど
```

Seleniumは，ブラウザを使って動的に巡回しつつ，スクレイピングをするのに適している．

JavascriptやPHPなどを使って，動的に作成されるサイトでは，URLだけではページを特定することはできない．
そのため，rvestだけではデータを取得するのが困難である．

## 動的サイトのスクレイピング

```{r, eval = FALSE}
  # アメリカの郵便番号はうまくいく
  # install.packages("chromote")
library(rvest)
library(chromote)
library(tidyverse)
url <- "https://www.fcc.gov/media/engineering/dtvmaps"
html <- read_html_live(url)
zip <- "30308"
html$type("#startpoint", zip)
html$click("#btnSub")
html$view()

html |> 
  html_table()


  # https://rvest.tidyverse.org/reference/LiveHTML.html
  # https://walkintheforest.net/r-rvest/
  # https://uchidamizuki.quarto.pub/blog/posts/2024/02/scraping-dynamic-sites-with-rvest.html
url <- "https://news.google.com/"
polite::bow(url)

  # scholar
url <- "https://scholar.google.co.jp/"
  # polite::bow(url)：OK
html <- read_html_live(url)
html$view()
html$type("#gs_hdr_tsi", "松村俊和")
html$click("#gs_hdr_tsb")
html$html_elements("a")


  # 郵便番号一覧の取得
  #   やっぱりあかん
library(rvest)
url <- "https://www.post.japanpost.jp/zipcode/"
  # polite::bow(url)：OK
html <- read_html_live(url)
zip <- "6580001"
html$type('[name="zip"]', zip)
html$click(".slim.sizeFull.btnTab")
html$view()
  # html |> html_table()

  # 国会図書館
  # ここもだめ
url <- "https://www.ndl.go.jp/"
  # polite::bow(url)：OK
html <- read_html_live(url)
html$type("#ndlsearchbox_top", "松村俊和")
  # クリックが効かない
html$click("#key_visual > div > div.kv_search_box.sp-none > div > form > div > input")
  # html$html_elements("#key_visual > div > div.kv_search_box.sp-none > div > form > div > input")
html$view()


  # 価格.com    https://qiita.com/kusano_t/items/3eda71e86d0835e460fc
  # ここもだめ
url <- "https://kakaku.com/"
  # polite::bow(url)：OK
html <- read_html_live(url)
html$type("#query", "rtx4060")
html$click("#main_search_button")
html$view()
html$html_elements("a")


  # 森北出版(途中までしかだめ)
library(rvest)
url <- "https://www.morikita.co.jp/"
html <- read_html_live(url)
  # html$view()
  # html$html_elements("a")
html$click(".modalWindow")
html$type("#keywords_modal", "ロケット")
  # html$html_elements(".btn._sizeM._colorMain")
html$click(".btn._sizeM._colorMain")
  # 検索まではできるが，その次が動かない
  # html$view()
  # html$html_elements("a")
html$type("#author", "テスト")
html$scroll_by(top = 200, left = 0)

  # googleはうまくいかない：変換予測が働くため?
url <- "https://google.co.jp/"
html <- read_html_live(url)
html$type('[name="q"]', "rvest")
html$click('name="btnK"')
html$view()
```
